---
layout: post
title:  "AudioNotch Post-Mortem"
date:   2017-09-04 15:55:55 -0400
categories: dev
published: false
---

It's that time of the year again! I'm going back to school tomorrow, and that marks the end of my summer. I'm feeling a little tired, to be honest. This summer, I had the least amount of rest I've ever had, and done the most work. More specifically, I worked as a software developer at [AudioNotch](https://audionotch.com). I'll break down what I did, and what I learned.

## What is AudioNotch?

![AudioNotch app banner]({{site.baseurl}}/img/audionotch/splash.png)

Briefly, let me explain to you what AudioNotch is. Essentially, it's a consumer SaaS that provides notched music for tinnitus sufferers.

Some of those words might be confusing. **Tinnitus** is a condition that creates a loud, high-pitched ringing noise in someone's ears following some sort of injury or damage to the ears/brain. While it's not exactly clear as to what causes tinnitus, many believe that it's the brain compensating for lack of sensory input. Unfortunately, there is no current catch-all cure.

Notched sound therapy takes the frequency that the tinnitus pitch is at and removes it from the frequency spectrum of a song, or "notching" it. It tricks the brain into removing the stimulus entirely. While it's not clear if notched sound therapy works for everybody, it works for some people.

![Diagram showing notching](https://www.audionotch.com/static/images/graph.png)

When I joined AudioNotch, it was in what the founder called "maintenance mode". No new features were being added, no visual updates, just fixing bugs and doing customer support. Unfortunately, their business had been tapering off a bit, in no part to its rather archaic website and the rise of free competitors. So, they brought me in to make a mobile app, and at the end, I was also able to update the look of the website.

## Mobile Development

The biggest undertaking I had to do was making a mobile app. Essentially, I had to make an iOS + Android app that replicated the functionality of the web app, while keeping the native feel.

Right off the bat, I wanted to make the app with [Ionic](https://ionicframework.com). Ionic is basically a themed Angular abstraction layer over [Cordova](https://cordova.apache.org/), which is a cross-platform app solution that works by running a browser on the phone and then making custom plugins to interact with native device features.

Wow, that was a mouthful. I think choosing Ionic was the right choice, especially with the timeframe that I had and the lack of experience I had with Swift/Android. I'll talk about some of the limitations I ran into with it later, but overall it ended up working well for me.

The core functionalities I had to implement were:
1. Account login/logout system that works with the web app's login/logout system
2. A tuner that replicated the [Web App's Tuner](https://audionotch.com/app/tune/)
3. The ability to create and play back notched music, similar to Step 2/3 on the Web App

I'll talk about how I did all three of these things, as well as some lessons I learned about app dev in general.

### Step 1: The Tuner

This was actually the first thing that I implemented code-wise, as it was one of the selling points of our product, especially on the mobile market. We also planned to make it free, as a leader into our paid service (notching). Basically, I had to make a page in our app that emitted a sound, with the user being able to change the frequency of the sound and apply filters to it.

The first thing I did was look into the [Javascript Web Audio API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API), with help from one of my bosses. Essentially, the Web Audio API lets you create and manipulate sounds, similar to audio manipulation libraries found in other programming languages. Unfortunately, the Web AudioAPI is still pretty new, and not all browsers support it. In addition, the documentation for it is sketchy at best, and some of the features don't work as intended.

I built the system with reference to the web version: since both the web version and the mobile version were made in Javascript, I could look to the web version for some help! Still, I had to convert everything to Typescript and clean up some of the code, plus fix quite a few bugs.

The spec of what I made was basically this:
* Tuner creates tone, generated by API
* User can control frequency of the tone
* User can control volume of the tone
* User can apply filters to the tone (tone types)
* User can see visualisation of frequency

First thing I did was make some sort of tone appear. You can do this pretty easily with the Web Audio API's `OscillatorNode`, which creates an oscillator that vibrates at a certain frequency; adding a slider that modifies that oscillator's frequency value is quite trivial. I hooked the `OscillatorNode` through a `GainNode`, that let me control the volume of the tone, and subsequently letting the user control it as well.

Adding the filters and the visualisations was a bit tougher. The default, unfiltered sound is just a pure tone, but I needed to add filters like "BB Noise", "Square Wave", or "Sawtooth". Luckily, the previous codebase from the web app helped. I implemented two different kinds of filters: tone filters, and noise filters. Tone filters changed the actual wave of the tone produced: changing these is as easy as changing a property on the oscillator, through `OscillatorNode.type`. Creating the noise filters was a bit harder. Instead of using the tone generated from the oscillator, I randomly generated white noise, and then applied a `BiquadFilterNode` (or a chain of them) creating a specific effect. The kind of filter I applied is called a bandpass filter: it only allows noise from a specific frequency through, and blocks out everything else. Due to the Web Audio API's newness, I didn't end up using the built-in bandpass, but rather running a lowpass and a highpass filter in parallel.

The visualisations were created by hooking an `AnalyserNode` into the sound loop, and sampling the volume of the sound at each frequency interval, and then drawing a bar corresponding to the volume on an HTML5 Canvas. It's actually easier than it sounds.

The foundations of what I was doing is called [Digital Signal Processing](https://en.wikipedia.org/wiki/Digital_signal_processing). It's quite a complex field, and ultimately I don't fully understand what I was doing. That being said, I did learn a bit about the field, and it's definitely interesting! One of my bosses was able to give me a quick run down, which was super nice of him, but it's a complex field that I'm excited to learn more about!

The nice thing about using Ionic in this situation was that I didn't have to implement the same feature in two different languages/platforms. While Java and Swift both have audio manipulation libraries, implementing every little feature in both languages would be a pain, especially if the implementation of each section of the tuner would require slightly different code.

Here's a gif of the tuner in action (though you can't hear the sound, unfortunately):

![Gif of mobile tuner]({{site.baseurl}}/img/audionotch/tuner.gif)

### Step 2: Create/Listen

The next thing I had to do was allow the user to create/listen to their notched music. This is kinda awkward, since we don't handle user auth yet, but since I was waiting on making changes to the web app I had to do this first. I ended up just creating a fake user/auth loop that always returned a test user's credentials, and then moved forward.

This was the hardest step of the entire operation. My original plan was to have the creation happen completely on the device, which would mean that I had to implement the filter locally. The notching step is basically a bandstop filter, where everything except a frequency band is let through. Unfortunately, there were quite a few obstacles in the way. The very first thing was that the default bandstop filter in the Web Audio API didn't fit what we needed to do: not only was it not powerful enough, it wasn't easily customizable. So, I tried making a bandstop filter by adding a lowpass and a highpass (exactly what they sound like) together. Unfortunately, they still weren't strong enough, even when creating 10 in a row and chaining them together. How many filters are chained together is called the "order" of the filter, and upon consulting my boss on the web app, I needed an order of about 1500. Programmatically generating that in Javascript, on a mobile phone nonetheless, would be near impossible (given computational constraints). I tried using some Javascript sound manipulation libraries such as [`fili.js`](https://github.com/markert/fili.js/), but unfortunately it still didn't work out.

Eventually, I figured out a different solution: send a POST request to the web app, which would do the grunt work of the audio processing, and then download the songs to the device. Seems quite simple, but I never really thought about it. Now, I bypass a lot of problems: since the computation is done on the server, I no longer have issues with mobile limitations. I also don't have to figure out the I/O for each of the mobile OS, as well as implementing a custom cordova plugin (which was my next idea).

There are a few problems with this solution: you need to be online, you don't have an idea of when the song is done processing (the web app endpoint wasn't that complex), users can't upload custom audio, and users can feel frustrated if they don't understand the system properly. Ultimately though, we were pushing an MVP, and this was the best solution.

So, I implemented an API endpoint for the AudioNotch Web App, which basically allowed me to send an auth request with operations (e.g. create a song, etc.). It ended up working out mostly fine, and that got me over the largest hump of the app development.

Listening was also pretty easy once I ditched the Web Audio API; originally, I tried loading in the data using `AudioContext.decodeAudioData()` from the Web Audio API, but the function is [literally broken](https://stackoverflow.com/questions/10365335/decodeaudiodata-returning-a-null-error) so I had to drop that idea as well. At the end, I used an [Ionic Native plugin](https://ionicframework.com/docs/native/) (a wrapper for cordova plugins, which allow you to access native mobile OS functionality) to load the files from the device's local storage and play them. That was pretty simple, even though [the documentation for the Ionic Native plugin is virtually non-existent.](https://ionicframework.com/docs/native/media/)

### Step 3: User/Auth

Now, it didn't make sense to make user/auth work last, but that's what I had to end up doing due to organisational problems. It was pretty simple: send a POST request to the server with the user/pass, and see if they were valid; if so, change the app's state to logged in. Things got complicated as I added different possible states though: what happens if the user is offline, or hasn't paid for an account yet? After some very confusing if-statements and promise chains, I figured out a system that I can't disclose here to figure everything out. Kinda.

### Surprise! IAPs!

While we were trying to publish the app, we also found out that with Apple's strict guidelines on app purchase guidelines we couldn't link to our web app's purchase page, and we HAD to have the user purchase our paid product through the App Store's In App Purchase method. I ended up using another Ionic Native plugin aptly named [InAppPurchase](https://ionicframework.com/docs/native/in-app-purchase/), but the documentation was absolutely awful: it took me a few days to get something that should've only taken a few hours to work properly. But, it worked. That's what mattered.

### Publishing

After a month, I had finally completed the mobile app. I went through the mess that was Ionic documentation and made a complete app, even if it did have quite a few bugs. We were ready to publish.

Publishing in the Google Play Store was actually super simple: all I had to do was sign the `.apk` file, pay for the account, answer a few questions, and voila! We were good to go! Our app was published on the day that I pressed the deploy button.

Unfortunately, the App Store wasn't the same. First, we had to fill out an insane amount of regulatory forms. Tax stuff, proving we're a business, etc. Secondly, the amount of review that we had to go through was insane! Each update had to be human-checked, and minor trip-ups required an entire submittal process. And the submittal process took a long time: I couldn't sign a version from the command line, but rather go through some long process in Xcode and fix my provisioning profiles (also pretty tough).

But, hey, it worked out. The app is on the [iOS](https://itunes.apple.com/us/app/audionotch/id1270247019?ls=1&mt=8) and [Android](https://play.google.com/store/apps/details?id=com.audionotch.audionotch) appstores, and we're making money. It all worked out, right?

## The Redesign

![Old AudioNotch website]({{site.baseurl}}/img/audionotch/index-old.png)

That website looks... a bit old. And that's because it is. When AudioNotch was originally

![New AudioNotch website]({{site.baseurl}}/img/audionotch/index-new.png)
